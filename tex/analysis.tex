\documentclass[../eva1_scion.tex]{subfiles}
\begin{document}
\section{Problem Analysis}\label{sec:analysis}
In this section, we will analyse the current  the global internet architecture and point out its current flaws and challenges. We establish a set of quality metrics and set the expectations regarding these metrics an internet architecture must meet in order to fulfil the requirements the modern scale of the Internet poses.

\subsection{The Global Internet}%
\label{ssec:the_global_internet}

Many protocols and technologies are involved in transporting information from point A to point B in a computer network. However, the core protocols and services which are responsible for communication between \textit{Autonomous System}s (AS)  and thus constitute what we will call the global internet, may be narrowed down to the following:

    \begin{itemize}
        \item Internet Protocol (IP): Provides addressing of interfaces and enables forwarding of data packets.
        \item Border Gateway Protocol (BGP): Provides route discovery between Autonomous Systems \cite{rfc_bgp}
        \item Domain Name System (DNS): Provides resolution between domain names and IP addresses.
        \item TLS Public Key Infrastructure (PKI): Provides cryptographic binding between names and an entity's public keys.
    \end{itemize}

\subsection{Quality Metrics}\label{ssec:quality_metrics}

Now that we know what set of technologies we include in the discussion, it must also be defined what measures of quality we are concerned with and what the expectations are regarding these quality measures. First and foremost we want to have availability  \textendash  if a resource is unreachable, all bets are off.  As a reference point for availability in a vital commination system, we might look at the \textit{plain old telephony system} (POTS). Its availability is often claimed to be 99.999 \% (or better) by telephone companies, which corresponds to about 0.86 second of down-time per day.

Once a resource is available, we want to be able to trust that resource. Trust is hard, manly because meaningful trust is a social and political concept which can only be \textit{conveyed} by technological means, not \textit{generated} by them. As humans, we expect that if any entity is revealed to be untrustworthy or becomes compromised, we can revoke our trust quickly. We also expect that we can choose whom to trust without any out-of-scale repercussions. The current solution to the trust problem is subject to the TLS PKI \cite{rfc_pki}, DNSSEC \cite{rfc_dnssec} and BGP Sec \cite{rfc_bgpsec}.

Now that trust is established, we would like to ensure that communication paths cannot be altered between two or more mutually trusted parties. This adds the requirement of data integrity. The current state of the global internet also suggest it would be wise to take scalability and efficiency into account as well. Adding and removing entities should be preferably low cost, quick, disruption free and error free. The same must be true for connections between entities.

\subsection{Shortcomings of the Current Internet Infrastructure}%
\label{ssec:shortcomings}

There are many factors that can degrade the service quality we expect from our internet connections; however, the following are the main issues which can and do affect the global internet daily:

    \begin{itemize}
        \item (Distributed) Denial of Service Attacks (DoS)
        \item Disruptions or poisoning of DNS
        \item BGP route misconfiguration or hijack
        \item Physical route failures
        \item Compromise or corruption of trust roots
    \end{itemize}

    Any disruptions or attacks on BGP, DNS or the PKI can cause major degradation of service quality for large parts of the global internet.

    The current protocols and services have only evolved little since their inception, which on one hand is a testament to the relative foresight and design rigour applied by their creators. On the other hand, are they no longer up to the task of managing the modern scale and complexity of the global internet and the current threat landscape. This becomes evident by the comparatively low availability of the internet. Uptime.com estimated the availability of the internet to around 99.84 \% for 2020 \cite{internet_availability}. This might seem high at first glance; however, this amounts to around 139 seconds per day. Of course, (D)DoS attacks and physical failures in the carrier medium are the most obvious causes for outages. Although, these tend to be often localized to one or just a few sites, only occasionally causing worldwide effects. In contrast, attacks on the BGP protocol like the route hijack against YouTube in 2006  by a Pakistani government ISP  \cite{youtube_hijack} can frequently cause outages on a global scale. Misconfigurations in BGP are typically of similar disruptiveness and have wide-ranging consequences as well. One recent example is a 6-hour complete outage at Facebook on October 4, 2021 \cite{facebook_oups}. Currently, there are no borders in place that would allow local containment of an issue.

    Even during normal operations, BGP can cause a disruption of service by temporary dead routes or routing loops. These can occur during route convergence, which can take tens of minutes in extreme cases \cite{route_convergence}.

    Managing trust is notoriously difficult. There have been multiple attempts to implement certificate revocation in the past. First with \textit{certificate revocation list} (CRL) \cite{rfc_crl} and then with \textit{online certificate status protocol} (OCSP) \cite{rfc_ocsp}. Both of these efforts are widely considered as failures. Not only revoking individual certificates is hard, removing compromised roots of trust is even harder and heavily relies on updating of browsers and operating systems. This can often take days or weeks, or may never happen at all in the case of IoT devices. Taking inspiration from human social behaviour, the natural thing to do may be to  drastically shrink the pool of trust roots one relies upon. However, doing this is almost an impossible task. First, the sheer number of available trust roots is immense. According to the EFF's SSL observatory, there is an estimated 650 \cite{trusted_entities} entities directly or indirectly trusted by both Microsoft and Mozilla and Firefox currently ships with around 150 CA root certificates included. Secondly, assessing them all reliably and continuously is an immense undertaking. Furthermore, removing a valid trust root, which an individual user deems untrustworthy, may render numerous resources on the internet untrusted and thus inaccessible. This serves to illustrate that the current trust model neither works nor  scales well.

    Finally, the question of scaleability and efficiency must be addressed. The current method by which available routes are propagated in BGP potentially requires a route-change to be propagated to every edge router of every AS on the entire internet. This leads to two problems: 1. Route convergence can take tens of minutes \cite{route_convergence} 2. Routing tables have become extremely large. In fact, routing tables have become so huge that router manufacturers are resorting to purpose-built memory hardware to optimize the longest prefix look-ups required for packet forwarding. This hardware is not only expensive, but also power hungry. This indicates that adding and removing ASes from the internet does not scale well and is expensive. Further, the process is error-prone and insecure, as demonstrated by numerous BGP misconfiguration related outages and route hijacking attacks.

    By now, the need for a profound change should have become evident. The current internet architecture does not or only partially provide the qualities its current scale and the surrounding threat landscape demand. Attempts to resolve these issues by evolution through grafting on solutions by extending existing protocols or replacing individual technologies have largely failed. Poorly adopted next-generation network technologies like IPv6 and DNSSEC clearly demonstrate this. Although technologies like TLS and BGPSec have seen partial success, they still suffer from lack of unsolved issues outside their problem scope or incomplete adoption. From this, it follows that a holistic solution \textendash revolution instead of evolution \textendash is needed \cite{xkcd_927}. SCION endeavours to deliver this whole cloth reengineering of the global internet architecture.

\end{document}
