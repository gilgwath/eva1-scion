\documentclass[../eva1_scion.tex]{subfiles}
\begin{document}
\chapter{Problem Analysis}\label{ch:analysis}

\section{The Global Interne}%
\label{sec:the_global_internet}

Many protocols and technologies are involved in transporting information from point A to point B in a computer network. However, the core protocols and services which make our modern connected world tick and thus constitute what we will call the global internet, may be narrowed down to the following:

\begin{itemize}
    \item Internet Protocol (IP): Provides adressing of devices.
    \item Border Gateway Protocol (BGP): Provides forwarding and path discovery between networks.
    \item Domain Name System (DNS): Provides resolution between domain names and ip addresses.
    \item Public Key Infrastructure (PKI): Provides cryptographic binding between names and entities.
\end{itemize}

\section{Quality Metrics}%
\label{sec:quality_metrics}

Now that we know what set of technologies we include in the discussion, it must also be definded what measures of quality we are concerned with and what the expectations are regarding these quality measures. Here again there are plenty of metrics to choos from. First and foremost we want to have availability - if a resource is unreachable, all bets are off. As a benchmark point for availablity in a vital cummications system we might look at the \textit{plain old telephon systems} (POTS). Its availablilty is generally estimated to be around 99.999 \% NEEDS REF.

If a resource is available we want to be able to trust that resource. Trust is hard, manly because trust is a social and political concept which can be only conveyed by techonlogical means, not generated by them. We expect that if any entity is reveiled to be untrustworthy or becomes compromised, we can revoke our trust quickly. We also expect that we can choos whom to trust with any out of scale reprecussions The current solution to the trust problem is subject of the browser PKI and BGP Sec.

Once trust is established we would like to ensure that communictain can not be altered between two or more mutualy trested parties. This adds the requirement of data integrity. The current state of the global internet also suggest it would be wise to take scalability and efficiency into account as well. Adding and removing entities should be preferably low cost, quick, disruption free and error free. The same must be true for connection between entities.

The possible reasons why the above qualities may be degraded are manyfold, but here are the ones that are at play on a global scale:

\section{Shortcommings of thc Current Internet Infrastructure}%
\label{sec:shortcommings}

There are many factors that can degrade the service quality we expect from our internet connections, however the following are the main issues which can and do affect the global internet on a daly basis.

\begin{itemize}
    \item (Distributed) Denial of Service Attacks (DOS)
    \item Disruptions or poisoning of DNS
    \item BGP route missconfiguration or high jack
    \item Physical route failures
    \item Compromise or corruption of trust roots
\end{itemize}

Any disruptions or attacks on BGP, DNS or the PKI can cause major degredation of service quality for larg parts of the global internet. These protocols and services have only evolved little, which on one hand is a testament to the relative foresight and designe rigour applied by their creators, on the other hand are they no longer up to task of managing todays scale and complexity of the global internet and the modern threat landscape. This becomes evident by the comparetativey low availablity of the internet. XY calculates the availability of the internet to around 99.9 \%. This might seem high at first glance, however this amounts to around xy seconds per day. Of course DOS attacks and physical failures in the carrier medium are most obvious causes for outages. However, these tend to be ofter localized to one or just a few sites, only ocassionaly causing world wide effects. In constrast, attacks on or misconfigurations in BGP often have wide ranging consequences and can take large swaths of the interrnet down. Even short lived problems like temporary dead routes or loops during route convergence often affect thousands of users.

Managing trust is notoriously dificult. There have been multiple attempts to implement certificate revocation in the past and all of them failed. The natural thing to do may be to then drastically shrink the pool of trust roots one relys upon. However doing this is almost impossible task. For once, the sheer number of available trust roots is immense. Firefox for example, at the time of wiriting, ships with 131 CA certificates includet NEEDS REF. Assessing them all relyably and continously is task. Furthermore, removing a trust root may render a large number of resources on the internet untrusted and thus inacessible. This also ilustrates that the current trust model does no scale.

Finally the question of scaleablity ond efficency must be addressed. The most glaring issue at the time is the exhaustion of IPv4 address space. IPv6 is a valiant effort to relieve this problem, which until now has not come to flurishion. This fact alone demonstrates how slowly core internet technologies evolve. Adding an new AS to the global internet not only requires each resource in that AS to be adressable by IP address it also requires that the new location of the AS to be propagated through the whole network to all the routers and to be added to their routing tables. As XY demostrates rout convergence in BGP can take up to XY seconds after a change, so changing, adding or removing routes is slow. Further is the process error prone, as demonstrated by noumerous BGP missconfiguration related outages. Prominant cases of BGP route high jacking further makes its lack of proper security clear.

By now the need for a profounde change should have become evident. The current internet architecture does not or only partially provide the qualties its current scale and the surrounding threat landscape demand. Attempts to resolve these issues by evolution through grafting on solutions by protocol extensions or replacing current individual technolgies have largely failed, as the current adoption of IPv6 and DNS Sec clearly demonstrate. Allthough techonologies like TLS and DNS Sec have seen partial sucess, they still suffer from lack of unsolved issues outside their problem scope. From this it follows that a wholistic solution - revolution instead of evolition - is needed. SCION endevours to deliver this whole cloth reenigineering. 
\end{document}
