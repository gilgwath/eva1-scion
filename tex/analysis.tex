\documentclass[../eva1_scion.tex]{subfiles}
\begin{document}
\chapter{Problem Analysis}\label{ch:analysis}

In this chapter we will analyse the current structure of the global internet architecture and point out it current flaws and challenges. We establish a set of quality metrics we expect from the global internet to meet our modern needs

\section{The Global Interne}%
\label{sec:the_global_internet}

Many protocols and technologies are involved in transporting information from point A to point B in a computer network. However, the core protocols and services which are responsible for inter AS  communication thus constitute what we will call the global internet, may be narrowed down to the following:

\begin{itemize}
    \item Internet Protocol (IP): Provides adressing of devices and enables forwarding of data packets.
    \item Border Gateway Protocol (BGP): Provides route discovery \ref{rfc_bgp}
    \item Domain Name System (DNS): Provides resolution between domain names and ip addresses.
    \item TLS Public Key Infrastructure (PKI): Provides cryptographic binding between names and an entities public keys.
\end{itemize}

\section{Quality Metrics}%
\label{sec:quality_metrics}

Now that we know what set of technologies we include in the discussion, it must also be definded what measures of quality we are concerned with and what the expectations are regarding these quality measures. Here again there are plenty of metrics to choose from. First and foremost we want to have availability - if a resource is unreachable, all bets are off. As a reference point for availablity in a vital cummications system we might look at the \textit{plain old telephon systems} (POTS). Its availablilty is generally estimated to be around 99.999 \% NEEDS REF.

If a resource is available we want to be able to trust that resource. Trust is hard, manly because meaningful trust is a social and political concept which can only be conveyed by techonlogical means, not generated by them. We expect that if any entity is reveiled to be untrustworthy or becomes compromised, we can revoke our trust quickly. We also expect that we can choose whom to trust with any out of scale reprecussions. The current solution to the trust problem is subject of the browser PKI and BGP Sec.

Once trust is established we would like to ensure that communictain can not be altered between two or more mutualy trested parties. This adds the requirement of data integrity. The current state of the global internet also suggest it would be wise to take scalability and efficiency into account as well. Adding and removing entities should be preferably low cost, quick, disruption free and error free. The same must be true for connection between entities.

The possible reasons why the above qualities may be degraded are manyfold, but here are the ones that are at play on a global scale:

\section{Shortcommings of the Current Internet Infrastructure}%
\label{sec:shortcommings}

There are many factors that can degrade the service quality we expect from our internet connections, however the following are the main issues which can and do affect the global internet on a daly basis.

\begin{itemize}
    \item (Distributed) Denial of Service Attacks (DoS)
    \item Disruptions or poisoning of DNS
    \item BGP route missconfiguration or high jack
    \item Physical route failures
    \item Compromise or corruption of trust roots
\end{itemize}

Any disruptions or attacks on BGP, DNS or the PKI can cause major degredation of service quality for larg parts of the global internet. Currently there are no borders in place which allow local containment of an issue.

The current protocols and services have only evolved little, which on one hand is a testament to the relative foresight and designe rigour applied by their creators, on the other hand are they no longer up to task of managing todays scale and complexity of the global internet and the modern threat landscape. This becomes evident by the comparetativey low availablity of the internet. XY calculates the availability of the internet to around 99.9 \%. This might seem high at first glance, however this amounts to around xy seconds per day. Of course (D)DoS attacks and physical failures in the carrier medium are most obvious causes for outages. However, these tend to be often localized to one or just a few sites, only ocassionaly causing world wide effects. In constrast, attacks on the BGP protocol like the route highjack carried out by the pakistani governement against YouTube in 2017 \ref{youtbe_highjack} can often cause outage on a global scale. Misconfigurations in BGP often are similary disruptive and have wide ranging consequences. One rescent example is a 6 hour complete outage at Facebook on the xy. October 2021 \ref{facebook_oups}.

Even during normal operations BGP can cause a distruption of service by temporary dead routes or routing loops, which can occoure during route convergence, which can take tens of minutes \ref{route_convergence}.

Managing trust is notoriously difficult. There have been multiple attempts to implement certificate revocation, first with \textit{certificate revocation list} (CRL) \ref{rfc_crl} and then with \textit{online certificate status protocol} (OCSP) \ref{rfc_opsc} in the past and all of them failed. Not only revoking individual certificates is hard, removing compromised roots of trust is even harder and heavily relies on updating of browsers and operating systems. Taking inspiration from human social behavior the natural thing to do may be to then drastically shrink the pool of trust roots one relys upon. However doing this is almost impossible task. For one, the sheer number of available trust roots is immense. There is an estimate of 3000 \ref{trusted_entities} trusted enties an and around 150 roots of trust in the current TLS PKI. For the onther, assessing them all relyably and continously is imense undertaking. Furthermore, removing a valid trust root which an individual user deems untrustworthy, my render a large number of resources on the internet untrusted and thus inacessible. This serves to illustrate that the current trust modle neither works, nor does it scale. 

Finally the question of scaleablity ond efficency must be addressed. The current method how available routes are propagated in BGP potentially requires a route change to be propagated to every edge router of every AS on the internet. This leads to two problems: 1. Route convergence can take tens of minutes \ref{route_convergence} 2. routing tables have become extremely large. Infact routing tables have become so larg that router manufacturer resort to purpos built memory hardware to optimize the longest prefix look-ups required for packet forwarding. This hardware is not only expensive, but also power hungry. This indicates that adding and removing ASes from the internet does not scale well and is expensive. Further, the process is error prone and insecure, as demonstrated by noumerous BGP missconfiguration related outages and route highjacking attacks.

By now the need for a profounde change should have become evident. The current internet architecture does not or only partially provide the qualties its current scale and the surrounding threat landscape demand. Attempts to resolve these issues by evolution through grafting on solutions by protocol extensions or replacing current individual technolgies have largely failed, as the current adoption of IPv6 and DNS Sec clearly demonstrate. Allthough techonologies like TLS and BGP Sec have seen partial sucess, they still suffer from lack of unsolved issues outside their problem scope. From this it follows that a wholistic solution - revolution instead of evolition - is needed. SCION endevours to deliver this whole cloth reenigineering. 
\end{document}
